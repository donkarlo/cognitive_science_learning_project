\section*{Major Thinkers Beyond Claude Shannon in Information Theory}
    
    \subsection*{1. Norbert Wiener (Cybernetics)}
        \textbf{Viewpoint:} Information as the basis of order and control.
        In \textit{Cybernetics} (1948), Wiener wrote that
        \emph{"Information is the measure of organization."}
        He treated information not merely as a mathematical quantity, but as a means to maintain stability in both living and mechanical systems.
        Wiener linked information to physical entropy: information flow corresponds to the preservation of order (negative entropy).
    
    \subsection*{2. Rolf Landauer (Physical Information)}
        \textbf{Viewpoint:} Information is physical and has energetic cost.
        Landauer formulated the principle that \emph{"Information is physical"} and proved in 1961 that erasing one bit of information dissipates at least $kT \ln 2$ energy.
        This connected information theory to thermodynamics and the foundations of physics.
    
    \subsection*{3. Edward Jaynes (Maximum Entropy Principle)}
        \textbf{Viewpoint:} Information as a rule for rational inference.
        Jaynes introduced the \textit{Maximum Entropy Principle}, stating that when only partial information is known,
        the most unbiased probability distribution is the one with maximum entropy.
        Information thus defines the rational boundary between what is known and unknown.
    
    \subsection*{4. Kolmogorov, Chaitin, and Solomonoff (Algorithmic Information Theory)}
        \textbf{Viewpoint:} Information as the length of the shortest possible description.
        They developed the concept of \textit{Kolmogorov Complexity},
        where the information content of a string is defined as the length of the shortest program that can generate it.
        Hence, regular patterns have low information; random data have high information.
    
    \subsection*{5. Luciano Floridi (Philosophy of Information)}
        \textbf{Viewpoint:} Information as a fundamental constituent of reality.
        Floridi, the founder of the \textit{Philosophy of Information}, argues that information is not just epistemic but ontological —
        a basic form of being in the digital age.
        He distinguishes four levels:
        \begin{itemize}
            \item Data
            \item Semantic Information
            \item Knowledge
            \item Wisdom
        \end{itemize}
    
    \subsection*{6. Karl Friston (Free Energy Principle)}
        \textbf{Viewpoint:} Information as prediction and uncertainty reduction in biological systems.
        Friston’s \textit{Free Energy Principle} builds on Shannon and statistical physics:
        the brain minimizes \emph{informational surprise} by continuously updating its internal model of the world.
        In this view, life itself can be described as the perpetual minimization of informational entropy.
    
    \subsection*{7. Other Notable Figures}
        \begin{itemize}
            \item \textbf{Thomas Cover} and \textbf{Joy Thomas}: Authors of the canonical text \textit{Elements of Information Theory}.
            \item \textbf{Claude Berrou}: Developer of turbo codes and the concept of ``soft information.''
            \item \textbf{John Archibald Wheeler}: Proposed the philosophical idea \textit{``It from bit''} — that physical reality arises from informational distinctions.
        \end{itemize}
    
    \subsection*{Summary Table}
        
        \begin{center}
            \begin{tabular}{|l|l|p{8cm}|}
                \hline
                \textbf{Name}        & \textbf{Domain}      & \textbf{View of Information}                                   \\
                \hline
                Claude Shannon       & Communication theory & Reduction of uncertainty about the message source              \\
                Norbert Wiener       & Cybernetics          & Medium of control and order in systems                         \\
                Rolf Landauer        & Physics              & Physical entity with energetic cost                            \\
                Edward Jaynes        & Statistics           & Principle of rational inference and uncertainty quantification \\
                Kolmogorov / Chaitin & Computation theory   & Shortest description length (algorithmic complexity)           \\
                Luciano Floridi      & Philosophy           & Foundational structure of reality and knowledge                \\
                Karl Friston         & Neuroscience         & Minimization of informational surprise (predictive brain)      \\
                \hline
            \end{tabular}
        \end{center}
        
        \bigskip
        \textit{In summary:}
        Shannon defined information as the \emph{reduction of uncertainty about the source of a message},
        while subsequent thinkers expanded it into physical, statistical, computational, philosophical, and cognitive domains.
